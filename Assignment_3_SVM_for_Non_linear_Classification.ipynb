{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaiDove1/Investment_Banking/blob/main/Assignment_3_SVM_for_Non_linear_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBq3yEPxSEWC"
      },
      "source": [
        "# Assignment 3: Non-Linear Classification with SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFMzEfKaDOdI"
      },
      "source": [
        "### CS 4774 Machine Learning - Department of Computer Science - University of Virginia\n",
        "\n",
        "In this assignment, you will implement your own version of SVM with kernels to classify non-linear data. For references, you may refer to my [lecture 10](https://drive.google.com/open?id=1CeBhepjDKBaFBq2BZq-zNQs-MC8ll7aL4qAF8TJ24FM) and [lecture 10b](https://drive.google.com/open?id=13BidUAs_c2QdZkf92axt2S748sbnbI9Hgxg-fzb-OuU) or Chapter 5 of the textbook if you need additional sample codes to help with your assignment. For deliverables, you must write code in Python and submit **this** Jupyter Notebook file (.ipynb) to earn a total of 100 pts.\n",
        "\n",
        "Note that you must save your Notebook filename under this format: **yourUvaUserId_assignment_3_svm.ipynb**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxGTRvPXTeDj"
      },
      "source": [
        "## 1. DATA SET AND VISUALIZATION FUNCTION\n",
        "We will use the non-linear toy data called the Moon dataset. You may use the code snippet below to generate the train/test set. Feel free to change the number of samples, and noise level. Additionally, a function plot_svm() is provided to help you visualize the decision boundary, margin, and support vectors on the dataset in 2D feature space.\n",
        "\n",
        "The provided function plot_svm() works out-the-box, and is the best way to visualize and evaluate the performance of your model. It assumes the classifier has an instance variable \"self.support_vectors_\", which is a numpy array of the support vectors found in training. DO NOT modify this function. Once your implementation in Task 3 is complete, the plots generated for your model should look similar to the plots generated for the standard library models in Task 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wht-_GHTXrn"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "X, y = make_moons (n_samples = 500, noise = 0.15, random_state = 49)\n",
        "y = y*2-1.0 # convert the labels from {0,1} to {-1, +1}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_svm (clf, X, y, axes=[-2, 3, -2, 2]):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of SVM including the decision boundary, margin, and its training data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    clf: your classifier handle\n",
        "    X: feature matrix shape(m_samples, n_features)\n",
        "    y: label vector shape(m_samples, )\n",
        "    axes: (optional) the axes of the plot in format [xmin, xmax, ymin, ymax]\n",
        "    \"\"\"\n",
        "    # Create a mesh grid based on the provided axes (100 x 100 resolution)\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s,x1s) # create a mesh grid\n",
        "    X_mesh = np.c_[x0.ravel(), x1.ravel()] # convert all mesh points into 2-D points\n",
        "    y_pred = clf.predict(X_mesh).reshape(x0.shape) # predict then covert back to the 2-D\n",
        "    y_decision = clf.decision_function(X_mesh).reshape(x0.shape)\n",
        "\n",
        "    plt.figsize=(16, 9)\n",
        "    plt.plot(X[:, 0][y==-1], X[:, 1][y==-1], \"bo\", label=\"Class -1\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"go\", label=\"Class +1\")\n",
        "    # Plot out the support vectors (in red)\n",
        "    plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=80, c=\"r\", label=\"Support Vectors\")\n",
        "    # Plot decision boundary and margins\n",
        "    plt.contourf(x0,x1, y_pred, cmap = plt.cm.brg, alpha = 0.1)\n",
        "    plt.contourf(x0,x1, y_decision, cmap = plt.cm.brg, alpha = 0.2)\n",
        "    plt.contour(x0, x1, y_decision, colors='k',\n",
        "                 levels=[-1, 0, 1], alpha=0.5,\n",
        "                 linestyles=['--', '-', '--'])\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.axis(\"auto\")\n",
        "\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_aV2q47DOdT"
      },
      "source": [
        "- - -\n",
        "## 2. TRAIN SVM FOR CLASSIFICATION TASK (20 pts)\n",
        "\n",
        "Use the standard libarary SVM classifier (SVC) on the training data, and then test the classifier on the test data. You will need to call SVM with 3 kernels: (1) Linear, (2) Polynomial and (3) Gaussian RBF.\n",
        "\n",
        "You should tune each model using a grid search or similar hyperparameter selection process, and report the best hyperparameters found. You will use these same hyperparameter settings later when testing and comparing to your implementation in Task 4. Once you've selected the best hyperparameters for each kernel, you will need to report the following:\n",
        "\n",
        "* Confusion matrix, Recall, and Precision. Discuss any tuning process on C and/or gamma to get to a reasonable result.\n",
        "* Use the provided plot_svm() to visualize the SVM in 2D. This might give you some insight on how SVM determines the margin and support vector on the Moon dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5yxy7kgcvZx"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce_XKlbNZvXX"
      },
      "source": [
        "## 3. IMPLEMENT YOUR OWN NON-LINEAR SVM (50 pts)\n",
        "Now that you see how the standard library SVM perform on the dataset, you will attempt to implement your own version of SVM. To help you, a template of SVM has been created including the quadratic optimization. Essensially, you will get the optimized value of $\\alpha$ for free. Note that there are **6 subtasks** which you need to implement in order to get the SVM to work properly.\n",
        "\n",
        "The provided code is extensively documented in comments, so that you may write code compatible with it. DO NOT edit the provided code, but read the comments and ask Piazza questions as necessary to understand it.\n",
        "\n",
        "Many of these tasks are made much easier by a working knowledge of numpy. If you have something you want to do, but are unsure how to do it in Python, consult the numpy documentation here: https://numpy.org/doc/1.17/reference/index.html. If you haven't built a Python class before, it may be worth scanning this tutorial: https://www.datacamp.com/community/tutorials/python-oop-tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ty2ssgfcw1E"
      },
      "source": [
        "# Use the information from the lecture slides to formulate the SVM Kernels.\n",
        "# To help you get started, the Linear Kernel (simply just a dot product) has been provided to you.\n",
        "# These kernel functions will be called in the SVM class\n",
        "# Linear Kernel\n",
        "def linear_kernel(u, v):\n",
        "    return np.dot(u, v)\n",
        "\n",
        "# Polynomial Kernel (of degree up to and including p)\n",
        "def polynomial_kernel(u, v, p=3):\n",
        "    # SUBTASK 1: Implement the kernel formulation here\n",
        "    return 0\n",
        "\n",
        "# Gaussian RBF Kernel\n",
        "def rbf_kernel(u, v, gamma=0.1):\n",
        "    # SUBTASK 2: Implement the kernel formulation here\n",
        "    # Note that gamma is provided, not sigma; see the slides for the relationship between gamma and sigma\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ocjh6xBSEMG"
      },
      "source": [
        "import cvxopt # The optimization package for Quadratic Programming\n",
        "import cvxopt.solvers\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "# extending these classes is required by all scikitlearn classes that will need fit, transform, and fit_transform functions\n",
        "# this is what enables inclusion in pipelines and compatability with other scikitlearn structures and functions\n",
        "\n",
        "class MySVM(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"The Implementation of the SVM class\"\"\"\n",
        "\n",
        "    def __init__(self, kernel=linear_kernel, C=None):\n",
        "        self.kernel = kernel # the kernel function used; this is a function and can be called\n",
        "        self.C = C # make sure to set this when instantiating this class; is C is None, your performance will be weird\n",
        "        if self.C is not None: self.C = float(self.C)\n",
        "        self.K = None\n",
        "        self.a = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Train SVM based on the training set\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: feature matrix shape(m_samples, n_features)\n",
        "        y: label vector shape(m_samples, )\n",
        "        \"\"\"\n",
        "\n",
        "        m_samples, n_features = X.shape\n",
        "\n",
        "        # To speed up repeated applications of the kernel function, dynamic programming is used\n",
        "        # All pairs of points have the kernel function computed over them and the result stored in K\n",
        "        # K is indexed by indices in X, so K[i,j] = kernel_function(X[i], X[j])\n",
        "        # You may use K in your code later in this function, but are not required to\n",
        "        # DO NOT edit this code\n",
        "        K = np.zeros((m_samples, m_samples))\n",
        "        for i in range(m_samples):\n",
        "            for j in range(m_samples):\n",
        "                K[i,j] = self.kernel(X[i], X[j])\n",
        "        self.K = K\n",
        "\n",
        "        # This part requires some understanding of Quadratic Programming (QP)\n",
        "        # Below is the user's guide for the QP from CVXOPT\n",
        "        # http://cvxopt.org/userguide/coneprog.html#quadratic-programming\n",
        "        # DO NOT edit any of this code until Subtask 3\n",
        "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
        "        q = cvxopt.matrix(np.ones(m_samples) * -1)\n",
        "        A = cvxopt.matrix(y, (1,m_samples))\n",
        "        b = cvxopt.matrix(0.0)\n",
        "\n",
        "        if self.C is None:\n",
        "            G = cvxopt.matrix(np.diag(np.ones(m_samples) * -1))\n",
        "            h = cvxopt.matrix(np.zeros(m_samples))\n",
        "        else:\n",
        "            tmp1 = np.diag(np.ones(m_samples) * -1)\n",
        "            tmp2 = np.identity(m_samples)\n",
        "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
        "            tmp1 = np.zeros(m_samples)\n",
        "            tmp2 = np.ones(m_samples) * self.C\n",
        "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
        "\n",
        "        # solve QP problem\n",
        "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "        # Lagrange multipliers for each point in X\n",
        "        a = np.ravel(solution['x'])\n",
        "\n",
        "        # Support vectors have non zero lagrange multipliers\n",
        "        # sv is a boolean array\n",
        "        # sv[i] is True iff a[i] is non-zero\n",
        "        sv = a > 1e-3\n",
        "\n",
        "        # SUBTASK 3: Find the support vectors\n",
        "        # note that plot_svm expects self.support_vectors_ to be a numpy array\n",
        "        # you can use a Python list in your implementation, as long as you cast to a numpy array at the end of this function\n",
        "        # or, you can cast to a numpy array now\n",
        "        # you should add lists to store any additional metadata related to each support vector necessary later\n",
        "        # for example, if you will use the Lagrange multipliers in future calculations...\n",
        "        # ...you should add an instance variable which lists the Langrage multipliers for each support vector\n",
        "        self.support_vectors_ = []\n",
        "\n",
        "        print(\"%d support vectors out of %d points\" % (sum(sv), m_samples))\n",
        "\n",
        "        if self.kernel == linear_kernel:\n",
        "            self.w = np.zeros(n_features)\n",
        "            # SUBTASK 4: Compute the Weight vector w\n",
        "        else:\n",
        "            # No need to compute for w in non-linear case\n",
        "            # Instead, we will use alpha (a) directly to predict the labels\n",
        "            self.w =  None\n",
        "\n",
        "        # SUBTASK 5: Compute the Intercept b\n",
        "        # b can be computed based on any arbitrary support vector\n",
        "        # it is recommended that you use self.support_vectors_[0], since this will always exist\n",
        "        # b should NOT be in terms of w, since w only exists in the linear case\n",
        "        self.b = 0\n",
        "\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"The decision function is essentially w^T . x + b\"\"\"\n",
        "        if self.w is not None:\n",
        "            return np.dot(X, self.w) + self.b\n",
        "        else:\n",
        "            # SUBTASK 6: for non-linear case, implement the kernel trick to predict label here\n",
        "            # you are predicting one label for each point in X\n",
        "            # note that this X may differ from the X passed to self.fit\n",
        "            y_predict = np.zeros(len(X))\n",
        "            return y_predict\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts -1,+1 based on the sign of the decision function\"\"\"\n",
        "        return np.sign(self.decision_function(X))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuI0EfvTJTIH"
      },
      "source": [
        "---\n",
        "## 4. COMPARE YOUR IMPLEMENTATION TO THE STANDARD LIBRARY (30 pts)\n",
        "Now that you have implemented your own SVM class, let's use it! Create 3 instances of your SVM class, each with a difference kernel (Linear, Polynomial, and RBF kernel), then train and test its performance in the Moon dataset as above. You may adjust hyperparameters to achieve high performance for both the standard library and your own implementations, and report the same metrics (confusion matrix, recall, and precision). You can use the plot_svm() function to visualize your SVM with decision boundary, margin, and support vectors on the dataset, and should plot each of the three instances of your class, to compare these plots with the standard library models' plots.\n",
        "\n",
        "Based on the number above, compare your SVM implementation with the standard library version. How did your SVM perform in comparison? Is there any major differences between the algorithms? If your performance is significantly worse, is there a different set of hyperparameters which better fits your model? Finally, reflect on your experience implementing a learning algorithm for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4ju2X6IUDt"
      },
      "source": [
        "# Your code here\n",
        "svm = MySVM()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpdLkbB_jcYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPjDDoYaDOdk"
      },
      "source": [
        "## Get Help?\n",
        "In case you get stuck in any step in the process, you may find some useful information from:\n",
        "\n",
        " * Consult my [lecture 10](https://drive.google.com/open?id=1CeBhepjDKBaFBq2BZq-zNQs-MC8ll7aL4qAF8TJ24FM) and [lecture 10b](https://drive.google.com/open?id=13BidUAs_c2QdZkf92axt2S748sbnbI9Hgxg-fzb-OuU) and/or the textbook\n",
        " * Talk to the TA, they are available and there to help you during office hour.\n",
        " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4774 Assignment 3:...\".\n",
        "\n",
        "Part of the codes used in this assignment is modified from Mathieu Blondel under the 3-Clause BSD License. Best of luck and have fun!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilvOF7PMStQp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}